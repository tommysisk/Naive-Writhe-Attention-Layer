{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0127427b-bd0c-4dbb-bec1-5858d62fe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "traj = md.load(\"/Users/tommysisk/asyn/lig47/fasudil_111frames.dcd\",\n",
    "               top=\"/Users/tommysisk/asyn/lig47/lig47.pdb\"\n",
    "              )\n",
    "traj = traj.atom_slice(traj.top.select(\"name CA\"))\n",
    "coords = traj.atom_slice(traj.top.select(\"name CA\")).xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ffbaf45-f3c5-4548-b7d0-00ccbfa58742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch_scatter import scatter\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data as GeometricData\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from functools import partial\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1235f-64a1-4ad2-a93a-0a0287d6bfbb",
   "metadata": {},
   "source": [
    " # Protein Residue stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "62d18174-02f3-4b6a-a458-0fed28c918b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_map(dic, keys):\n",
    "    check = list(dic.keys())\n",
    "    assert all(k in check for k in keys), \"Not all keys exist in dict\"\n",
    "    return list(map(dic.__getitem__, keys))\n",
    "\n",
    "single_letter_codes = [\"G\", \"A\", \"S\", \"P\", \"V\", \"T\", \"C\", \"L\", \"I\",\"N\",\n",
    "                       \"D\", \"Q\", \"K\", \"E\", \"M\", \"H\", \"F\", \"R\", \"Y\", \"W\"]\n",
    "\n",
    "three_letter_codes = [\"GLY\",\"ALA\",\"SER\",\"PRO\",\"VAL\",\"THR\",\"CYS\",\"LEU\",\"ILE\",\"ASN\",\n",
    "                      \"ASP\",\"GLN\",\"LYS\",\"GLU\",\"MET\",\"HIS\",\"PHE\",\"ARG\",\"TYR\",\"TRP\"]\n",
    "\n",
    "abr_to_code_ = dict(zip(three_letter_codes, single_letter_codes))\n",
    "\n",
    "code_to_index_ = dict(zip(single_letter_codes, range(len(single_letter_codes))))\n",
    "\n",
    "def get_codes(traj):\n",
    "    return list(map(str, list(traj.top.to_fasta())[0]))\n",
    "    \n",
    "def abr_to_code(keys):\n",
    "    return dict_map(abr_to_code, keys)\n",
    "\n",
    "def code_to_index(codes):\n",
    "    if len(codes[0]) > 1:\n",
    "        codes = abr_to_code(codes)\n",
    "    return torch.LongTensor(dict_map(code_to_index_, codes))\n",
    "\n",
    "\n",
    "\n",
    "def get_residue_bonds(code_sequence):\n",
    "    indices = code_to_index(code_sequence)\n",
    "    residue_bond_types_ = torch.arange(400).long().reshape(20, 20)\n",
    "    return residue_bond_types_[indices[:-1], indices[1:]]\n",
    "\n",
    "code_sequence = get_codes(traj)\n",
    "index_sequence = code_to_index(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438685a1-81b5-45a7-9b17-f239f0d02471",
   "metadata": {},
   "source": [
    " # Radial Basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "f8755692-d409-406e-bdbc-d657bc2da2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SoftUnitStep(torch.autograd.Function):\n",
    "    # pylint: disable=arguments-differ\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x) -> torch.Tensor:\n",
    "        ctx.save_for_backward(x)\n",
    "        y = torch.zeros_like(x)\n",
    "        m = x > 0.0\n",
    "        y[m] = (-1 / x[m]).exp()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy) -> torch.Tensor:\n",
    "        (x,) = ctx.saved_tensors\n",
    "        dx = torch.zeros_like(x)\n",
    "        m = x > 0.0\n",
    "        xm = x[m]\n",
    "        dx[m] = (-1 / xm).exp() / xm.pow(2)\n",
    "        return dx * dy\n",
    "\n",
    "\n",
    "def soft_unit_step(x):\n",
    "    r\"\"\"smooth :math:`C^\\infty` version of the unit step function\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        x \\mapsto \\theta(x) e^{-1/x}\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    .. jupyter-execute::\n",
    "        :hide-code:\n",
    "\n",
    "        import torch\n",
    "        from e3nn.math import soft_unit_step\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        x = torch.linspace(-1.0, 10.0, 1000)\n",
    "        plt.plot(x, soft_unit_step(x));\n",
    "    \"\"\"\n",
    "    return _SoftUnitStep.apply(x)\n",
    "        \n",
    "        \n",
    "def soft_one_hot_linspace(x: torch.Tensor, start, end, number, basis=None, cutoff=None) -> torch.Tensor:\n",
    "    r\"\"\"Projection on a basis of functions\n",
    "\n",
    "    Returns a set of :math:`\\{y_i(x)\\}_{i=1}^N`,\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y_i(x) = \\frac{1}{Z} f_i(x)\n",
    "\n",
    "    where :math:`x` is the input and :math:`f_i` is the ith basis function.\n",
    "    :math:`Z` is a constant defined (if possible) such that,\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\langle \\sum_{i=1}^N y_i(x)^2 \\rangle_x \\approx 1\n",
    "\n",
    "    See the last plot below.\n",
    "    Note that ``bessel`` basis cannot be normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    start : float\n",
    "        minimum value span by the basis\n",
    "\n",
    "    end : float\n",
    "        maximum value span by the basis\n",
    "\n",
    "    number : int\n",
    "        number of basis functions :math:`N`\n",
    "\n",
    "    basis : {'gaussian', 'cosine', 'smooth_finite', 'fourier', 'bessel'}\n",
    "        choice of basis family; note that due to the :math:`1/x` term, ``bessel`` basis does not satisfy the normalization of\n",
    "        other basis choices\n",
    "\n",
    "    cutoff : bool\n",
    "        if ``cutoff=True`` then for all :math:`x` outside of the interval defined by ``(start, end)``,\n",
    "        :math:`\\forall i, \\; f_i(x) \\approx 0`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `torch.Tensor`\n",
    "        tensor of shape :math:`(..., N)`\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    .. jupyter-execute::\n",
    "        :hide-code:\n",
    "\n",
    "        import torch\n",
    "        from e3nn.math import soft_one_hot_linspace\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        bases = ['gaussian', 'cosine', 'smooth_finite', 'fourier', 'bessel']\n",
    "        x = torch.linspace(-1.0, 2.0, 100)\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        fig, axss = plt.subplots(len(bases), 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "\n",
    "        for axs, b in zip(axss, bases):\n",
    "            for ax, c in zip(axs, [True, False]):\n",
    "                plt.sca(ax)\n",
    "                plt.plot(x, soft_one_hot_linspace(x, -0.5, 1.5, number=4, basis=b, cutoff=c))\n",
    "                plt.plot([-0.5]*2, [-2, 2], 'k-.')\n",
    "                plt.plot([1.5]*2, [-2, 2], 'k-.')\n",
    "                plt.title(f\"{b}\" + (\" with cutoff\" if c else \"\"))\n",
    "\n",
    "        plt.ylim(-1, 1.5)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        fig, axss = plt.subplots(len(bases), 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "\n",
    "        for axs, b in zip(axss, bases):\n",
    "            for ax, c in zip(axs, [True, False]):\n",
    "                plt.sca(ax)\n",
    "                plt.plot(x, soft_one_hot_linspace(x, -0.5, 1.5, number=4, basis=b, cutoff=c).pow(2).sum(1))\n",
    "                plt.plot([-0.5]*2, [-2, 2], 'k-.')\n",
    "                plt.plot([1.5]*2, [-2, 2], 'k-.')\n",
    "                plt.title(f\"{b}\" + (\" with cutoff\" if c else \"\"))\n",
    "\n",
    "        plt.ylim(0, 2)\n",
    "        plt.tight_layout()\n",
    "    \"\"\"\n",
    "    # pylint: disable=misplaced-comparison-constant\n",
    "\n",
    "    if cutoff not in [True, False]:\n",
    "        raise ValueError(\"cutoff must be specified\")\n",
    "\n",
    "    if not cutoff:\n",
    "        values = torch.linspace(start, end, number, dtype=x.dtype, device=x.device)\n",
    "        step = values[1] - values[0]\n",
    "    else:\n",
    "        values = torch.linspace(start, end, number + 2, dtype=x.dtype, device=x.device)\n",
    "        step = values[1] - values[0]\n",
    "        values = values[1:-1]\n",
    "\n",
    "    diff = (x[..., None] - values) / step\n",
    "\n",
    "    if basis == \"gaussian\":\n",
    "        return diff.pow(2).neg().exp().div(1.12)\n",
    "\n",
    "    if basis == \"cosine\":\n",
    "        return torch.cos(math.pi / 2 * diff) * (diff < 1) * (-1 < diff)\n",
    "\n",
    "    if basis == \"smooth_finite\":\n",
    "        return 1.14136 * torch.exp(torch.tensor(2.0)) * soft_unit_step(diff + 1) * soft_unit_step(1 - diff)\n",
    "\n",
    "    if basis == \"fourier\":\n",
    "        x = (x[..., None] - start) / (end - start)\n",
    "        if not cutoff:\n",
    "            i = torch.arange(0, number, dtype=x.dtype, device=x.device)\n",
    "            return torch.cos(math.pi * i * x) / math.sqrt(0.25 + number / 2)\n",
    "        else:\n",
    "            i = torch.arange(1, number + 1, dtype=x.dtype, device=x.device)\n",
    "            return torch.sin(math.pi * i * x) / math.sqrt(0.25 + number / 2) * (0 < x) * (x < 1)\n",
    "\n",
    "    if basis == \"bessel\":\n",
    "        x = x[..., None] - start\n",
    "        c = end - start\n",
    "        bessel_roots = torch.arange(1, number + 1, dtype=x.dtype, device=x.device) * math.pi\n",
    "        out = math.sqrt(2 / c) * torch.sin(bessel_roots * x / c) / x\n",
    "\n",
    "        if not cutoff:\n",
    "            return out\n",
    "        else:\n",
    "            return out * ((x / c) < 1) * (0 < x)\n",
    "\n",
    "    raise ValueError(f'basis=\"{basis}\" is not a valid entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278f55c-27b3-45ed-91c5-632947a10500",
   "metadata": {},
   "source": [
    " # Writhe stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "debb851e-50ac-4110-878e-2442d17ab317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(x: np.ndarray, y: np.ndarray):\n",
    "    return np.asarray(list(itertools.product(x, y)))\n",
    "\n",
    "\n",
    "def combinations(x):\n",
    "    return np.asarray(list(itertools.combinations(x, 2)))\n",
    "\n",
    "\n",
    "def shifted_pairs(x: np.ndarray, shift: int, ax: int = 1):\n",
    "    return np.stack([x[:-shift], x[shift:]], ax)\n",
    "\n",
    "\n",
    "def get_segments(n: int = None,\n",
    "                 length: int = 1,\n",
    "                 index0: np.ndarray = None,\n",
    "                 index1: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Function to retrieve indices of segment pairs for various use cases.\n",
    "    Returns an (n_segment_pairs, 4) array where each row (quadruplet) contains : (start1, end1, start2, end2)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if all(i is None for i in (index0, index1)):\n",
    "        assert n is not None, \\\n",
    "            \"Must provide indices (index0:array, (optionally) index1:array) or the number of points (n: int)\"\n",
    "        segments = combinations(shifted_pairs(np.arange(n), length)).reshape(-1, 4)\n",
    "        return torch.from_numpy(segments[~(segments[:, 1] == segments[:, 2])])\n",
    "\n",
    "    else:\n",
    "        assert index0 is not None, (\"If providing only one set of indices, must set the index0 argument \\n\"\n",
    "                                    \"Cannot only supply the index1 argument (doesn't make sense in this context\")\n",
    "        if index1 is not None:\n",
    "            return torch.from_numpy(product(*[shifted_pairs(i, length) for i in (index0, index1)]).reshape(-1, 4))\n",
    "        else:\n",
    "            segments = combinations(shifted_pairs(index0, length)).reshape(-1, 4)\n",
    "            return torch.from_numpy(segments[~(segments[:, 1] == segments[:, 2])])\n",
    "\n",
    "\n",
    "##########################################   fastest ways of implementing these linear algebra ops for this purpose  (NOT trivial) ############################################\n",
    "\n",
    "\n",
    "    \n",
    "def nnorm(x: torch.Tensor):\n",
    "    \n",
    "    \"\"\"Convenience function for (batched) normalization of vectors stored in arrays with last dimension 3\"\"\"\n",
    "    \n",
    "    norm = torch.linalg.norm(x, axis=-1)\n",
    "    \n",
    "    if x.ndim == 4:\n",
    "        return x / norm[:, :, :, None]\n",
    "    elif x.ndim == 3:\n",
    "        return x / norm[:, :, None]\n",
    "    elif x.ndim == 2:\n",
    "        return x / norm[:, None]\n",
    "    else:\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "def ncross(x: torch.Tensor, y: torch.Tensor):\n",
    "\n",
    "    \"\"\"Convenience function for (batched) cross products of vectors stored in arrays with last dimension 3\"\"\" \n",
    "\n",
    "    # c = np.array(list(map(cross,x,y)))\n",
    "    c = torch.cross(x, y, axis=-1)\n",
    "    return c\n",
    "\n",
    "\n",
    "def ndot(x, y):\n",
    "\n",
    "    \"\"\"Convenience function for (batched) dot products of vectors stored in arrays with last dimension 3\"\"\"\n",
    "\n",
    "    # d = np.array(list(map(dot,x,y)))[:,None]\n",
    "    d = torch.sum(x * y, axis=-1)\n",
    "    return d\n",
    "\n",
    "\n",
    "def ndet(v1, v2, v3):\n",
    "    \"\"\"for the triple product and finding the signed sin of the angle between v2 and v3, v1 should\n",
    "    be set equal to a vector mutually orthogonal to v2,v3\"\"\"\n",
    "    #     det = np.array(list(map(lambda x,y,z:np.linalg.det(np.array([x,y,z])),\n",
    "    #                         v1,v2,v3)))[:,None]\n",
    "    det = ndot(v1, ncross(v2, v3))\n",
    "    return det\n",
    "\n",
    "\n",
    "def uproj(a, b, norm_b: bool = True):\n",
    "    \"\"\"Convenience function for (batched) othogonal projection of vectors stored in arrays with last dimension 3\n",
    "    where a is a set of vectors which we be othogonally projected onto a single (batched) set of vectors, b\"\"\"\n",
    "    b = nnorm(b) if norm_b else b\n",
    "    # faster than np.matmul when using ray\n",
    "    return a - b * torch.sum(a[:, None, :] * b[:, None, :], -1).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "solid_angle = lambda a, b: -torch.arcsin(torch.prod(nnorm(uproj(a, b)), 1).sum(-1).clip(-1, 1))\n",
    "# or arccos(...) - (np.pi / 2)\n",
    "# slower than current version with ray\n",
    "# Usage of solid angle in writhe computation (not as time efficient as implementation in use)\n",
    "# indices = zip([0, 1, 0, 2],\n",
    "#               [3, 2, 3, 1],\n",
    "#               [1, 3, 2, 0])\n",
    "\n",
    "# omega = np.stack([solid_angle(displacements[:, [i, j]], displacements[:, None, n])\n",
    "#                   for i, j, n in indices], 1).squeeze().sum(-1)\n",
    "##############################################################################################################################\n",
    "\n",
    "\n",
    "def writhe_segment(segment=None, xyz=None, smat=None):\n",
    "    \"\"\"compute the writhe (signed crossing) of 2 segments for all frames (index 0) in xyz (xyz can contain just one frame)\n",
    "    \n",
    "    THERE ARE 2 INPUT OPTIONS\n",
    "    \n",
    "    **provide both of the following**\n",
    "    \n",
    "    segment: numpy array of shape (4,) giving the indices of the 4 alpha carbons in xyz creating 2 segments:::\n",
    "             array([seg1_start_point,seg1_end_point,seg2_start_point,seg2_end_point])\n",
    "\n",
    "    xyz: numpy array of shape (Nframes, N_alpha_carbons, 3),coordinate array giving the positions of ALL the alpha carbons\n",
    "    \n",
    "    **OR just the following**\n",
    "    \n",
    "    smat ::: numpy array of shape (Nframes, 4, 3) : sliced coordinate matrix: coordinate array that is pre-sliced\n",
    "    with only the positions of the 4 alpha carbons constituting the 2 segments to compute the writhe between \"\"\"\n",
    "\n",
    "    if smat is None:\n",
    "        assert not ((segment is None) or (xyz is None)), \\\n",
    "            \"must input smat or both a segment and xyz coordinates\"\n",
    "        smat = (xyz[None, :, :] if xyz.ndim < 3 else xyz)[:, segment]\n",
    "    else:\n",
    "        smat = smat[None, :, :] if smat.ndim < 3 else smat\n",
    "\n",
    "    # smat = nnorm(smat)\n",
    "    sum_dim = None if smat.shape[0] == 1 else 1\n",
    "\n",
    "    # broadcasting trick\n",
    "    # negative sign, None placement and order are intentional, don't change without testing equivalent option\n",
    "    displacements = nnorm((-smat[:, :2, None, :] + smat[:, None, 2:, :]).reshape(-1, 4, 3))\n",
    "\n",
    "    # array broadcasting is (surprisingly) slower than list comprehensions\n",
    "    # when using ray for the following operations (without ray, broadcasting should be faster).\n",
    "\n",
    "    crosses = nnorm(ncross(displacements[:, [0, 1, 3, 2]], displacements[:, [1, 3, 2, 0]]))\n",
    "\n",
    "    omega = torch.arcsin(ndot(crosses[:, [0, 1, 2, 3]], crosses[:, [1, 2, 3, 0]]).clip(-1, 1)).squeeze().sum(sum_dim)\n",
    "\n",
    "    signs = torch.sign(ndot(ncross(nnorm(smat[:, 3] - smat[:, 2]),\n",
    "                                nnorm(smat[:, 1] - smat[:, 0])),\n",
    "                         displacements[:, 0])).squeeze()\n",
    "\n",
    "    wr = (1 / (2 * torch.pi)) * (omega * signs)\n",
    "\n",
    "    return wr\n",
    "\n",
    "\n",
    "def writhe_segments_along_axis(segments: torch.LongTensor, xyz: torch.Tensor, axis: int = 1):\n",
    "    \"\"\"helper function for parallelization to compute writhe over chuncks of segments for all frames in xyz\"\"\"\n",
    "    # noinspection PyTypeChecker\n",
    "    return torch.stack([writhe_segment(segment, xyz, None) for segment in segments], 1)\n",
    "    \n",
    "# Unfinished, need analouge to cpu treatment with ray parallelization\n",
    "\n",
    "# def calc_writhe_parallel(segments: np.ndarray, xyz: np.ndarray,) -> \"Nframes by Nsegments np.ndarray\":\n",
    "#     \"\"\"parallelize writhe calculation by segment, avoids making multiple copies of coordinate (xyz) matrix,\n",
    "#     uses torch.scatter to parallelize\"\"\"\n",
    "\n",
    "#     # TODO finish remaking this function to scatter the computation across multiple GPUs\n",
    "    \n",
    "#     #n_split = 1 if torch.cuda.device_count() == 0 else torch.cuda.device_count()\n",
    "#     chunks = torch.tensor_split(segments, int(torch.cuda.device_count()))\n",
    "#     result = torch.concatenate([writhe_segments_along_axis(segments=chunk, xyz=xyz_ref) for chunk in chunks]).T.squeeze()\n",
    "#     return result\n",
    "\n",
    "\n",
    "def to_writhe_adj_matrix(writhe_features,\n",
    "                         n_points,\n",
    "                         length,\n",
    "                         segments=None,\n",
    "                         full_matrix=True):\n",
    "    \n",
    "    n = len(writhe_features)\n",
    "\n",
    "    if segments is None:\n",
    "        segments = get_segments(n_points, length)\n",
    "\n",
    "    adj_matrix = torch.zeros((n, n_points, n_points))\n",
    "\n",
    "    adj_matrix[:, segments[:, 0], segments[:, 2]] = writhe_features\n",
    "    adj_matrix[:, segments[:, 1], segments[:, 3]] = writhe_features\n",
    "    adj_matrix = adj_matrix + adj_matrix.swapaxes(1, 2) if full_matrix else adj_matrix\n",
    "\n",
    "    return adj_matrix.squeeze()\n",
    "\n",
    "#def to_writhe_pair_list(writhe_features, n_point, length, segments):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714b082-f90d-439e-bafb-872357f4d883",
   "metadata": {},
   "source": [
    " # Writhe Layer that can easily integrate into existing cpaiNN source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ac534776-e1a3-4775-84f5-fa97a0ff9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchWrithe(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute writhe for a set of coordinates. \n",
    "    Return an (batch, n_atoms, n_atoms) writhe 'adjacentcy' matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_atoms: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"segments_\", get_segments(n_atoms))\n",
    "        self.register_buffer(\"n_atoms_\", torch.LongTensor([n_atoms]))\n",
    "    \n",
    "    @property\n",
    "    def n_atoms(self):\n",
    "        return self.n_atoms_.item()\n",
    "\n",
    "    @property\n",
    "    def segments(self):\n",
    "        return self.segments_\n",
    "    \n",
    "    def to_matrix(self, x):\n",
    "        return to_writhe_adj_matrix(x, self.n_atoms, 1, self.segments)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.to_matrix(writhe_segments_along_axis(self.segments, x.reshape(-1, self.n_atoms, 3)))\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple implementation of self attention where we skip the 'value' projection and return only the attention logits (positive)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        self.key = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        #self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        #values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        #weighted = torch.bmm(attention, values)\n",
    "        return attention\n",
    "\n",
    "class WritheEmbedding(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed each value of writhe by a super position of a basis set of functions (vectors).\n",
    "    The weight of each function in the super position is determined by a soft one hot embedding (RBF) of each value of writhe.\n",
    "    OUTPUT : (batch, n_atoms, n_atoms, bins)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, bins: int=300, basis: str = \"gaussian\", cutoff: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.soft_one_hot = partial(soft_one_hot_linspace,\n",
    "                                    start=-1,\n",
    "                                    end=1,\n",
    "                                    number=bins,\n",
    "                                    basis=basis,\n",
    "                                    cutoff=cutoff)\n",
    "        \n",
    "        std = 1. / math.sqrt(embed_dim)\n",
    "\n",
    "        self.register_parameter(\"functions\", torch.nn.Parameter(torch.Tensor(1, 1, 1, bins, embed_dim).uniform_(-std, std),\n",
    "                                                                requires_grad=True)\n",
    "                               )\n",
    "        \n",
    "        #self.functions = nn.Parameter(torch.Tensor(1, 1, 1, bins, embed_dim).uniform_(-std, std), requires_grad=True)\n",
    "    \n",
    "    def get_weights(self, x):\n",
    "        return self.soft_one_hot(x).unsqueeze(-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (self.get_weights(x) * self.functions).sum(-2)\n",
    "        \n",
    "        \n",
    "\n",
    "class WritheEmbeddedAttention(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    1) Compute writhe from a set of coordinates.\n",
    "    2) Use Radial Basis Functions to embed each value of writhe the same dimension as node embedding.\n",
    "    3) Compute attention logits from node embeddings.\n",
    "    4) Weight embedded writhe values with attention logits and sum for each node with every other node. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 node_embed_dim: int,\n",
    "                 bins: int,\n",
    "                 n_atoms: int,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.writhe = TorchWrithe(n_atoms)\n",
    "        self.attn = SelfAttention(node_embed_dim)\n",
    "        self.embed = WritheEmbedding(embed_dim=node_embed_dim, bins=bins)\n",
    "    \n",
    "    def forward(self, node_embeddings: torch.Tensor, xyz: torch.Tensor):\n",
    "        attn = self.attn(node_embeddings).unsqueeze(-1)\n",
    "        writhe_embed = self.embed(self.writhe(xyz))\n",
    "        return node_embeddings + (attn * writhe_embed).sum(1)\n",
    "\n",
    "\n",
    "\n",
    "class AddEmbeddedWrithe(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to add writhe embeddings as edge features to batch of graphs.\n",
    "    Follows structure of other feature additions in ITO classes. \n",
    "    1) Compute writhe from a set of coordinates.\n",
    "    2) Use Radial Basis Functions to embed each value of writhe the same dimension as node embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 bins: int,\n",
    "                 n_atoms: int,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.writhe = TorchWrithe(n_atoms)\n",
    "        self.embed = WritheEmbedding(embed_dim=embed_dim, bins=bins)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        # indexing trick to ensure writhe values are correct\n",
    "        index = batch.edge_index[0] // self.writhe.n_atoms\n",
    "        index_src, index_dst = batch.edge_index % self.writhe.n_atoms\n",
    "        \n",
    "        writhe_embed = self.embed(self.writhe(batch.x))\n",
    "        batch.writhe = writhe_embed[index, index_src, index_dst, :]\n",
    "        return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62f39b-2f86-4efc-b30e-3a000b76b8aa",
   "metadata": {},
   "source": [
    " # cpaiNN source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "7ff9ca86-f15b-4fe9-889b-390da6b8ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, f_in, f_hidden, f_out, skip_connection=False):\n",
    "        super().__init__()\n",
    "        self.skip_connection = skip_connection\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(f_in, f_hidden),\n",
    "            torch.nn.LayerNorm(f_hidden),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(f_hidden, f_hidden),\n",
    "            torch.nn.LayerNorm(f_hidden),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(f_hidden, f_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.skip_connection:\n",
    "            return x + self.mlp(x)\n",
    "\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class AddEdgeIndex(torch.nn.Module):\n",
    "    def __init__(self, n_neighbors=None, cutoff=None):\n",
    "        super().__init__()\n",
    "        self.n_neighbors = n_neighbors if n_neighbors else 10000\n",
    "        self.cutoff = cutoff if cutoff is not None else float(\"inf\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = batch.clone()\n",
    "        edge_index = self.generate_edge_index(batch)\n",
    "        batch.edge_index = edge_index.to(batch.x.device)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class AddSpatialEdgeFeatures(torch.nn.Module):\n",
    "    def forward(self, batch, *_, **__):\n",
    "        r = batch.x[batch.edge_index[0]] - batch.x[batch.edge_index[1]]\n",
    "\n",
    "        edge_dist = r.norm(dim=-1)\n",
    "        edge_dir = r / (1 + edge_dist.unsqueeze(-1))\n",
    "\n",
    "        batch.edge_dist = edge_dist\n",
    "        batch.edge_dir = edge_dir\n",
    "        return batch\n",
    "\n",
    "\n",
    "class InvariantFeatures(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement embedding in child class\n",
    "    All features that will be embedded should be in the batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_name, type_=\"node\"):\n",
    "        super().__init__()\n",
    "        self.feature_name = feature_name\n",
    "        self.type = type_\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embedded_features = self.embedding(batch[self.feature_name])\n",
    "\n",
    "        name = f\"invariant_{self.type}_features\"\n",
    "        if hasattr(batch, name):\n",
    "            batch[name] = torch.cat([batch[name], embedded_features], dim=-1)\n",
    "        else:\n",
    "            batch[name] = embedded_features\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class NominalEmbedding(InvariantFeatures):\n",
    "    def __init__(self, feature_name, n_features, n_types, feature_type=\"node\"):\n",
    "        super().__init__(feature_name, feature_type)\n",
    "        self.embedding = torch.nn.Embedding(n_types, n_features)\n",
    "\n",
    "\n",
    "class DeviceTracker(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"device_tracker\", torch.tensor(1))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.device_tracker.device\n",
    "\n",
    "\n",
    "class PositionalEncoder(DeviceTracker):\n",
    "    def __init__(self, dim, length=10):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"dim must be even for positional encoding for sin/cos\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.length = length\n",
    "        self.max_rank = dim // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        encodings = [self.positional_encoding(x, rank) for rank in range(self.max_rank)]\n",
    "        return torch.cat(\n",
    "            encodings,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    def positional_encoding(self, x, rank):\n",
    "        sin = torch.sin(x / self.length * rank * np.pi)\n",
    "        cos = torch.cos(x / self.length * rank * np.pi)\n",
    "        assert (\n",
    "            cos.device == self.device\n",
    "        ), f\"batch device {cos.device} != model device {self.device}\"\n",
    "        return torch.stack((cos, sin), axis=1)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(InvariantFeatures):\n",
    "    def __init__(self, feature_name, n_features, length):\n",
    "        super().__init__(feature_name)\n",
    "        assert n_features % 2 == 0, \"n_features must be even\"\n",
    "        self.rank = n_features // 2\n",
    "        self.embedding = PositionalEncoder(n_features, length)\n",
    "\n",
    "\n",
    "class CombineInvariantFeatures(torch.nn.Module):\n",
    "    def __init__(self, n_features_in, n_features_out):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP(n_features_in, n_features_out, n_features_out)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch.invariant_node_features = self.mlp(batch.invariant_node_features)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class AddEquivariantFeatures(DeviceTracker):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, batch):\n",
    "        eq_features = torch.zeros(\n",
    "            batch.batch.shape[0],\n",
    "            self.n_features,\n",
    "            3,\n",
    "        )\n",
    "        batch.equivariant_node_features = eq_features.to(self.device)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class PaiNNTLScore(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features=32,\n",
    "        embedding_layers=2,\n",
    "        score_layers=5,\n",
    "        max_lag=1000,\n",
    "        diff_steps=1000,\n",
    "        n_types=167,\n",
    "        dist_encoding=\"positional_encoding\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            embedding.AddSpatialEdgeFeatures(),\n",
    "            embedding.NominalEmbedding(\n",
    "                \"bonds\", n_features, n_types=4, feature_type=\"edge\"\n",
    "            ),\n",
    "            embedding.NominalEmbedding(\"atoms\", n_features, n_types=n_types),\n",
    "            embedding.AddEquivariantFeatures(n_features),\n",
    "            #  embedding.CombineInvariantFeatures(2 * n_features, n_features),\n",
    "            PaiNNBase(\n",
    "                n_features=n_features,\n",
    "                n_features_out=n_features,\n",
    "                n_layers=embedding_layers,\n",
    "                dist_encoding=dist_encoding,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.embed = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            embedding.AddSpatialEdgeFeatures(),\n",
    "            embedding.PositionalEmbedding(\"ts_diff\", n_features, diff_steps),\n",
    "            embedding.PositionalEmbedding(\"lag\", n_features, max_lag),\n",
    "            embedding.CombineInvariantFeatures(3 * n_features, n_features),\n",
    "            PaiNNBase(\n",
    "                n_features=n_features,\n",
    "                dist_encoding=dist_encoding,\n",
    "                n_layers=score_layers,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        cond = batch[\"cond\"].clone().to(self.device)\n",
    "        corr = batch[\"corr\"].clone().to(self.device)\n",
    "\n",
    "        batch_idx = batch[\"cond\"].batch\n",
    "        corr.lag = batch[\"lag\"][batch_idx].squeeze()\n",
    "        corr.ts_diff = batch[\"ts_diff\"][batch_idx].squeeze()\n",
    "\n",
    "        embedded = self.embed(cond)\n",
    "        corr.invariant_node_features = embedded.invariant_node_features\n",
    "        corr.equivariant_node_features = embedded.equivariant_node_features\n",
    "        corr.invariant_edge_features = embedded.invariant_edge_features\n",
    "        corr.edge_index = embedded.edge_index\n",
    "\n",
    "        dx = self.net(corr).equivariant_node_features.squeeze()\n",
    "\n",
    "        corr.x += dx\n",
    "        return corr\n",
    "\n",
    "\n",
    "class PaiNNBase(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features=128,\n",
    "        n_layers=5,\n",
    "        n_features_out=1,\n",
    "        length_scale=10,\n",
    "        dist_encoding=\"positional_encoding\",\n",
    "        use_edge_features=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                Message(\n",
    "                    n_features=n_features,\n",
    "                    length_scale=length_scale,\n",
    "                    dist_encoding=dist_encoding,\n",
    "                    use_edge_features=use_edge_features,\n",
    "                )\n",
    "            )\n",
    "            layers.append(Update(n_features))\n",
    "\n",
    "        layers.append(Readout(n_features, n_features_out))\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)\n",
    "\n",
    "\n",
    "class Message(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features=128,\n",
    "        length_scale=10,\n",
    "        dist_encoding=\"positional_encoding\",\n",
    "        use_edge_features=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.use_edge_features = use_edge_features\n",
    "\n",
    "        assert dist_encoding in (\n",
    "            a := [\"positional_encoding\", \"soft_one_hot\"]\n",
    "        ), f\"positional_encoder must be one of {a}\"\n",
    "\n",
    "        if dist_encoding in [\"positional_encoding\", None]:\n",
    "            self.positional_encoder = embedding.PositionalEncoder(\n",
    "                n_features, length=length_scale\n",
    "            )\n",
    "        elif dist_encoding == \"soft_one_hot\":\n",
    "            self.positional_encoder = embedding.SoftOneHotEncoder(\n",
    "                n_features, max_radius=length_scale\n",
    "            )\n",
    "\n",
    "        phi_in_features = 2 * n_features if use_edge_features else n_features\n",
    "        self.phi = embedding.MLP(phi_in_features, n_features, 4 * n_features)\n",
    "        self.w = embedding.MLP(n_features, n_features, 4 * n_features)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        src_node = batch.edge_index[0]\n",
    "        dst_node = batch.edge_index[1]\n",
    "\n",
    "        in_features = batch.invariant_node_features[src_node]\n",
    "\n",
    "        if self.use_edge_features:\n",
    "            in_features = torch.cat(\n",
    "                [in_features, batch.invariant_edge_features], dim=-1\n",
    "            )\n",
    "\n",
    "        positional_encoding = self.positional_encoder(batch.edge_dist)\n",
    "\n",
    "        gates, scale_edge_dir, ds, de = torch.split(\n",
    "            self.phi(in_features) * self.w(positional_encoding),\n",
    "            self.n_features,\n",
    "            dim=-1,\n",
    "        )\n",
    "        gated_features = multiply_first_dim(\n",
    "            gates, batch.equivariant_node_features[src_node]\n",
    "        )\n",
    "        scaled_edge_dir = multiply_first_dim(\n",
    "            scale_edge_dir, batch.edge_dir.unsqueeze(1).repeat(1, self.n_features, 1)\n",
    "        )\n",
    "\n",
    "        dv = scaled_edge_dir + gated_features\n",
    "        dv = scatter(dv, dst_node, dim=0)\n",
    "        ds = scatter(ds, dst_node, dim=0)\n",
    "\n",
    "        batch.equivariant_node_features += dv\n",
    "        batch.invariant_node_features += ds\n",
    "        batch.invariant_edge_features += de\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def multiply_first_dim(w, x):\n",
    "    with warnings.catch_warnings(record=True):\n",
    "        return (w.T * x.T).T\n",
    "\n",
    "\n",
    "class Update(torch.nn.Module):\n",
    "    def __init__(self, n_features=128):\n",
    "        super().__init__()\n",
    "        self.u = EquivariantLinear(n_features, n_features)\n",
    "        self.v = EquivariantLinear(n_features, n_features)\n",
    "        self.n_features = n_features\n",
    "        self.mlp = embedding.MLP(2 * n_features, n_features, 3 * n_features)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        v = batch.equivariant_node_features\n",
    "        s = batch.invariant_node_features\n",
    "\n",
    "        vv = self.v(v)\n",
    "        uv = self.u(v)\n",
    "\n",
    "        vv_norm = vv.norm(dim=-1)\n",
    "        vv_squared_norm = vv_norm**2\n",
    "\n",
    "        mlp_in = torch.cat([vv_norm, s], dim=-1)\n",
    "\n",
    "        gates, scale_squared_norm, add_invariant_features = torch.split(\n",
    "            self.mlp(mlp_in), self.n_features, dim=-1\n",
    "        )\n",
    "\n",
    "        delta_v = multiply_first_dim(uv, gates)\n",
    "        delta_s = vv_squared_norm * scale_squared_norm + add_invariant_features\n",
    "\n",
    "        batch.invariant_node_features = batch.invariant_node_features + delta_s\n",
    "        batch.equivariant_node_features = batch.equivariant_node_features + delta_v\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class EquivariantLinear(torch.nn.Module):\n",
    "    def __init__(self, n_features_in, n_features_out):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(n_features_in, n_features_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x.swapaxes(-1, -2)).swapaxes(-1, -2)\n",
    "\n",
    "\n",
    "class Readout(torch.nn.Module):\n",
    "    def __init__(self, n_features=128, n_features_out=13):\n",
    "        super().__init__()\n",
    "        self.mlp = embedding.MLP(n_features, n_features, 2 * n_features_out)\n",
    "        self.V = EquivariantLinear(  # pylint:disable=invalid-name\n",
    "            n_features, n_features_out\n",
    "        )\n",
    "        self.n_features_out = n_features_out\n",
    "\n",
    "    def forward(self, batch):\n",
    "        invariant_node_features_out, gates = torch.split(\n",
    "            self.mlp(batch.invariant_node_features), self.n_features_out, dim=-1\n",
    "        )\n",
    "\n",
    "        equivariant_node_features = self.V(batch.equivariant_node_features)\n",
    "        equivariant_node_features_out = multiply_first_dim(\n",
    "            equivariant_node_features, gates\n",
    "        )\n",
    "\n",
    "        batch.invariant_node_features = invariant_node_features_out\n",
    "        batch.equivariant_node_features = equivariant_node_features_out\n",
    "        return batch\n",
    "\n",
    "\n",
    "class PaiNNScore(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features=32,\n",
    "        score_layers=3,\n",
    "        diff_steps=1000,\n",
    "        n_types=167,\n",
    "        dist_encoding=\"positional_encoding\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            embedding.AddSpatialEdgeFeatures(),\n",
    "            embedding.NominalEmbedding(\n",
    "                \"bonds\", n_features, n_types=4, feature_type=\"edge\"\n",
    "            ),\n",
    "            embedding.NominalEmbedding(\"atoms\", n_features, n_types=n_types),\n",
    "            embedding.PositionalEmbedding(\"ts_diff\", n_features, diff_steps),\n",
    "            embedding.AddEquivariantFeatures(n_features),\n",
    "            embedding.CombineInvariantFeatures(2 * n_features, n_features),\n",
    "            PaiNNBase(\n",
    "                n_features=n_features,\n",
    "                n_features_out=1,\n",
    "                n_layers=score_layers,\n",
    "                dist_encoding=dist_encoding,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        corr = batch[\"corr\"].clone().to(self.device)\n",
    "        batch_idx = batch[\"corr\"].batch\n",
    "        corr.ts_diff = batch[\"ts_diff\"][batch_idx].squeeze()\n",
    "\n",
    "        dx = self.net(corr).equivariant_node_features.squeeze()\n",
    "        corr.x += dx\n",
    "\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712ab58-922e-4693-840c-86a507b65000",
   "metadata": {},
   "source": [
    " # Add RBF Embedded Writhe into cpainNN work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c3422f1c-4d0a-44aa-bb83-365d98968a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[200, 3], edge_index=[2, 1900], atoms=[200], bonds=[190], batch=[200], ptr=[11], edge_dist=[1900], edge_dir=[1900, 3], invariant_edge_features=[190, 10], invariant_node_features=[200, 10], equivariant_node_features=[200, 10, 3], writhe=[1900, 10])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make data objects\n",
    "data_objs = [GeometricData(x=torch.Tensor(x),\n",
    "                           atoms=index_sequence,\n",
    "                           edge_index=torch.triu_indices(20, 20, 1).long(),\n",
    "                           bonds=get_residue_bonds(code_sequence),)\n",
    "            for x in coords]\n",
    "\n",
    "dataset = GraphDataSet(data_objs)\n",
    "loader = DataLoader(dataset, batch_size=10)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# make input batch and add features including new writhe feature\n",
    "n_features = 10\n",
    "n_bond_types = 400\n",
    "n_residue_types = 20\n",
    "layers = [AddSpatialEdgeFeatures(),\n",
    "          NominalEmbedding(\"bonds\", n_features, n_types=n_bond_types, feature_type=\"edge\"),\n",
    "          NominalEmbedding(\"atoms\", n_features, n_types=n_residue_types),\n",
    "          AddEquivariantFeatures(n_features),\n",
    "          AddEmbeddedWrithe(embed_dim=n_features, bins=100, n_atoms=20)\n",
    "         ]\n",
    "for layer in layers:\n",
    "    batch = layer(batch)\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a744f-2151-4628-aa62-3ee3c7c103c1",
   "metadata": {},
   "source": [
    " # Prep inputs for testing SE3 Equivariant attention implementation in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f2ca9ec6-f335-4d71-8400-a980456cfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_features = dict(vectors=batch.equivariant_node_features, scalars=batch.invariant_node_features.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4917216e-6368-4dd8-9113-d2900e935c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_features_with_edge = dict(vectors=batch.equivariant_node_features,\n",
    "                                    scalars=batch.invariant_node_features.unsqueeze(-1),\n",
    "                                    edges=batch.writhe.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe4a0b-e88a-4c6b-85ff-a928e4f8e9dd",
   "metadata": {},
   "source": [
    " # SE3 Equivariant attention pytorch implementation\n",
    "\n",
    " # The class names say SO(3) but with the addition of cross products / determinants we get SE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "5e752431-8469-4d11-a154-b0994f1ca740",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dict(vectors=vectors, scalars=scalars)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_dim, out_dim), nn.LayerNorm(out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SODInvariantScalars(nn.Module):\n",
    "    \"\"\" NN parameterized SO(d)-invariant scalar function.\n",
    "    g(VV^T, scalars, minors(V)) -> scalar\n",
    "    \"\"\"\n",
    "    def __init__(self, n_vectors: int,\n",
    "                 n_scalars: int,\n",
    "                 out_dim: int):\n",
    "        \"\"\"\n",
    "        :param n_vectors: channels of input vector features\n",
    "        :param n_scalars: channels of input scalar features\n",
    "        :param out_dim: dimension of the output feature\n",
    "        \"\"\"\n",
    "        super(SODInvariantScalars, self).__init__()\n",
    "        self.n_vectors = n_vectors\n",
    "        self.n_scalars = n_scalars\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        triu_idx = torch.triu_indices(n_vectors, n_vectors) # get unique values from GRAHAM matrix\n",
    "        assert triu_idx.shape[1] == int(n_vectors * (n_vectors + 1) / 2)\n",
    "        self.register_buffer('triu_idx', triu_idx)\n",
    "\n",
    "        sub_idx = torch.combinations(torch.arange(n_vectors), 3, False) # get all possible determiniants of vector triples\n",
    "        \n",
    "        if sub_idx.shape[0] != 0:\n",
    "            self.register_buffer('sub_idx', torch.flatten(sub_idx))\n",
    "        else:\n",
    "            self.register_buffer('sub_idx', None)\n",
    "\n",
    "        self.in_dim = n_scalars + self.triu_idx.shape[1] + sub_idx.shape[0] # add up dims of dot prodicts, cross products, and scalar features \n",
    "        \n",
    "        self.net = MLP(self.in_dim, self.out_dim)\n",
    "\n",
    "        # NOTE : the vector features only show themselves in the form of dots and crosses\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_determinant(A: torch.Tensor):\n",
    "        \"\"\" Compute the determinant of batched 3x3 matrices\n",
    "        :param A: ..., 3, 3\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = A[..., 0, 0] * A[..., 1, 1] * A[..., 2, 2] + \\\n",
    "                 A[..., 0, 1] * A[..., 1, 2] * A[..., 2, 0] + \\\n",
    "                 A[..., 0, 2] * A[..., 1, 0] * A[..., 2, 1] - \\\n",
    "                 A[..., 0, 2] * A[..., 1, 1] * A[..., 2, 0] - \\\n",
    "                 A[..., 0, 1] * A[..., 1, 0] * A[..., 2, 2] - \\\n",
    "                 A[..., 0, 0] * A[..., 1, 2] * A[..., 2, 1]\n",
    "        return output\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        :param features: dict,\n",
    "            'vec': B, n_vectors, 3\n",
    "            'scalar': B, n_scalars, 1\n",
    "        :return:\n",
    "            B, out_dim\n",
    "        \"\"\"\n",
    "\n",
    "        vectors, scalars = features.values()\n",
    "        \n",
    "        net_input_features = []\n",
    "        \n",
    "\n",
    "        # dot products\n",
    "        B = vectors.shape[0]\n",
    "        inner_product = torch.bmm(vectors, vectors.transpose(2, 1)) # GRAHAM matrix\n",
    "        inner_product = inner_product[..., self.triu_idx[0], self.triu_idx[1]]  # unique indices including diagonal\n",
    "        net_input_features.append(inner_product)\n",
    "\n",
    "        # determinants\n",
    "        det_mat = torch.index_select(vectors, 1, self.sub_idx)\n",
    "        det_mat = det_mat.reshape(B, -1, 3, 3)\n",
    "        determinants = self._compute_determinant(det_mat)\n",
    "        net_input_features.append(determinants)\n",
    "\n",
    "        # scalar features -> go directly into input with no changes\n",
    "        scalars = scalars[...,0]\n",
    "        net_input_features.append(scalars)\n",
    "\n",
    "        # concatenate results\n",
    "        net_input_features = torch.cat(net_input_features, dim=-1)\n",
    "\n",
    "        # pass through neural net\n",
    "        scalar_weights = self.net(net_input_features)  # B, out_dim\n",
    "\n",
    "        # will return a (batch, out_dim) result\n",
    "\n",
    "        return scalar_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "2daf349a-5b57-4d7d-b8f8-24e70cafebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4049, -0.1439, -0.5524,  ..., -0.1914,  0.2513, -0.6142],\n",
       "        [-0.5472, -0.2710, -0.4868,  ...,  0.1545,  0.1505, -0.7519],\n",
       "        [-0.7155, -0.4028, -0.0791,  ..., -0.3790,  0.0344, -0.4619],\n",
       "        ...,\n",
       "        [-0.5246, -0.2488,  0.1019,  ...,  0.2331, -0.7568, -0.6138],\n",
       "        [-0.7155, -0.4028, -0.0791,  ..., -0.3790,  0.0344, -0.4619],\n",
       "        [-0.8707, -0.4729,  0.1103,  ..., -0.3236,  0.0674, -0.3891]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SODInvariantScalars(n_vectors=n_features, n_scalars=n_features, out_dim=n_features)(attention_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9525e2bf-0257-4947-b276-ccea57149e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3LayerNorm(nn.Module):\n",
    "    def __init__(self, feature_dims):\n",
    "        \"\"\"\n",
    "        :param feature_dims: dict, channels of input vectors and scalars\n",
    "        \"\"\"\n",
    "        super(SO3LayerNorm, self).__init__()\n",
    "\n",
    "        self.feature_dims = feature_dims\n",
    "        self.LN_modules = nn.ModuleDict()\n",
    "        self.eps = 1e-12\n",
    "\n",
    "        for data_type, channel in feature_dims.items():\n",
    "            if channel != 0:\n",
    "                self.LN_modules[data_type] = nn.LayerNorm(channel)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        \"\"\"\n",
    "               :param features: dict\n",
    "                   'vectors': B, m_in['vectors'], 3\n",
    "                   'scalar': B, m_in['scalars'], 1\n",
    "               :param kwargs:\n",
    "               :return: dict\n",
    "                   'vectors': B, m_in['vectors'], 3\n",
    "                   'scalar', B, m_in[scalars'], 1\n",
    "               \"\"\"\n",
    "        new_features = {}\n",
    "        \n",
    "        if 'vectors' in self.LN_modules:\n",
    "            data_item = features['vectors']     # B, m_in[*], dim\n",
    "            norm = torch.sqrt(torch.sum(torch.square(data_item), dim=-1) + self.eps)  # B, m_in[*]\n",
    "            phase = data_item / norm.unsqueeze(-1)  # B, m_in[*], dim\n",
    "            transformed = self.LN_modules['vectors'](norm)  # B, m_in[*]\n",
    "            new_features[\"vectors\"] = transformed.unsqueeze(-1) * phase  # B, m_in[*], dim\n",
    "        \n",
    "        if 'scalars' in self.LN_modules:\n",
    "            data_item = features['scalars'][..., 0]  # B, m_in[*]\n",
    "            data_item = self.LN_modules['scalars'](data_item)    # B, m_in[*]\n",
    "            new_features[\"scalars\"] = data_item.unsqueeze(-1)    # B, m_in[*], 1\n",
    "\n",
    "        return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0a95ad96-cb17-41e7-87f9-18a12a057074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors': tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]], grad_fn=<MulBackward0>),\n",
       " 'scalars': tensor([[[-1.0887],\n",
       "          [ 2.0282],\n",
       "          [ 0.4680],\n",
       "          ...,\n",
       "          [ 0.9983],\n",
       "          [ 0.0679],\n",
       "          [ 0.0903]],\n",
       " \n",
       "         [[ 1.9933],\n",
       "          [ 1.3789],\n",
       "          [-0.1465],\n",
       "          ...,\n",
       "          [-0.6675],\n",
       "          [-0.8803],\n",
       "          [ 0.6322]],\n",
       " \n",
       "         [[ 0.7891],\n",
       "          [ 0.5154],\n",
       "          [-1.1109],\n",
       "          ...,\n",
       "          [-0.7488],\n",
       "          [ 1.0056],\n",
       "          [-0.3290]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3336],\n",
       "          [-1.4145],\n",
       "          [ 0.6350],\n",
       "          ...,\n",
       "          [-1.0703],\n",
       "          [ 0.3605],\n",
       "          [ 0.6748]],\n",
       " \n",
       "         [[ 0.7891],\n",
       "          [ 0.5154],\n",
       "          [-1.1109],\n",
       "          ...,\n",
       "          [-0.7488],\n",
       "          [ 1.0056],\n",
       "          [-0.3290]],\n",
       " \n",
       "         [[ 1.7460],\n",
       "          [ 1.2830],\n",
       "          [ 0.1309],\n",
       "          ...,\n",
       "          [-1.0283],\n",
       "          [ 0.8775],\n",
       "          [-0.4777]]], grad_fn=<UnsqueezeBackward0>)}"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SO3LayerNorm(feature_dims=dict(vectors=n_features, scalars=n_features))(attention_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "946f56e9-aabc-41e6-aa87-11ddad7b1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3EquivariantVector(nn.Module):\n",
    "    \"\"\"SO(3) Equivariant Vector Function. Possibly update scalar vectors, depending on whether\n",
    "    the output channels are zeros or not.\n",
    "    The updated scalar vector is extracted from the output of ODInvariantScalars\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_vectors: int,\n",
    "                 n_vectors_out: int,\n",
    "                 n_scalars: int,\n",
    "                 n_scalars_out: int,\n",
    "                 cross_product: bool = True,\n",
    "                 input_LN: bool = False):\n",
    "        \"\"\"\n",
    "        :param n_vectors: channels of input vector features\n",
    "        :param n_vectors_out: channels of output vector features\n",
    "        :param n_scalars: channels of input scalar features\n",
    "        :param n_scalars_out: channels of output scalar features\n",
    "        :param cross_product: whether or not to compute the cross product (defaults to true for SE3 equivariance)\n",
    "        :param input_LN: normalize the input vectors and scalars individually (bool)\n",
    "        \"\"\"\n",
    "        super(SO3EquivariantVector, self).__init__()\n",
    "        assert (n_vectors_out == 0) or (n_vectors >0)\n",
    "\n",
    "        self.n_vectors = n_vectors\n",
    "        self.n_vectors_out = n_vectors_out\n",
    "        self.n_scalars = n_scalars\n",
    "        self.n_scalars_out = n_scalars_out\n",
    "        \n",
    "        \n",
    "        # cross product information\n",
    "        if cross_product:\n",
    "            self.cross_product = True\n",
    "        else:\n",
    "            self.cross_product = False\n",
    "        \n",
    "        if self.cross_product:\n",
    "            # later, we'll need a respresentation that gives a pair of vectors for each unique cross product possible\n",
    "            # we cross those -> len(tri_up) normal vectors -> concatenate 10 equivariant vectors\n",
    "            self.n_cross_prod = int(n_vectors * (n_vectors - 1) / 2) \n",
    "        \n",
    "        else:\n",
    "            self.n_cross_prod = 0\n",
    "\n",
    "        cross_prod_idx = torch.triu_indices(n_vectors, n_vectors, offset=1)\n",
    "        self.register_buffer('cross_prod_idx', cross_prod_idx)\n",
    "        self.normalize_term = self.n_cross_prod + self.n_vectors\n",
    "        \n",
    "        self.out_dim_vec = self.n_vectors_out * (self.n_cross_prod + self.n_vectors) # n * (n**2 + n) / 2\n",
    "        self.out_dim_s = self.n_scalars_out\n",
    "        \n",
    "        self.out_dim = self.out_dim_vec + self.out_dim_s # after passing through scalar net, we still want a new scalar reprensentation\n",
    "        self.scalar_net = SODInvariantScalars(self.n_vectors, self.n_scalars, self.out_dim)\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        self.input_LN = input_LN\n",
    "        if self.input_LN:\n",
    "            self.input_layer_norm = SO3LayerNorm(feature_dims=dict(vectors=n_vectors, scalars=n_scalars))\n",
    "        \n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        :param features: dict\n",
    "            'vec': B, n_vectors, 3\n",
    "            'scalar': B, n_scalars, 1 (optional)\n",
    "        :return: dict\n",
    "            'vec': B, n_vectors_out, 3\n",
    "            'scalar': B, n_scalars_out, 1\n",
    "        \"\"\"\n",
    "        # normalize vectors and scalars individually\n",
    "\n",
    "        #vectors, scalars = features.values()\n",
    "        \n",
    "        if self.input_LN:\n",
    "            features = self.input_layer_norm(features)\n",
    "        \n",
    "        \n",
    "        # pass through SE3Equivariant scalar network\n",
    "        weights = self.scalar_net(features)  # B, out_dim\n",
    "        \n",
    "        # split output out SE3Equivariant scalar net, the output\n",
    "        \n",
    "        # the scalar weights received from the split remain unmodified throughout the rest of the forward method and are returned as is\n",
    "\n",
    "        vector_weights, scalar_weights = torch.split(weights, [self.out_dim_vec, self.out_dim_s], dim=-1)\n",
    "        \n",
    "        new_features = {}\n",
    "\n",
    "        if self.out_dim_vec > 0:\n",
    "            B = features[\"vectors\"].shape[0]\n",
    "            vector_weights = vector_weights.reshape(B, self.n_vectors_out, self.n_cross_prod + self.n_vectors)\n",
    "            \n",
    "            # if there are vectors, we compute the paired cross products\n",
    "            if self.n_vectors > 1 and self.cross_product:\n",
    "                \n",
    "                # prepare matrcies for cross products\n",
    "                c0, c1 = self.cross_prod_idx # separate indices\n",
    "\n",
    "                # separate paratitions of vectors for cross product computation\n",
    "                mat0 = torch.gather(features[\"vectors\"], 1, c0[None, :, None].expand(B, -1, 3))  # B, n_cross_prod, 3\n",
    "                mat1 = torch.gather(features[\"vectors\"], 1, c1[None, :, None].expand(B, -1, 3))  # B, n_cross_prod, 3\n",
    "\n",
    "                \n",
    "                # compute cross products\n",
    "                cross_prods = torch.linalg.cross(mat0, mat1, dim=-1)  # B, n_cross_prod, 3\n",
    "\n",
    "                \n",
    "                # combine vectors and cross products to get a larger set of vectors, still 3D\n",
    "                cat_mat = torch.cat([features[\"vectors\"], cross_prods], dim=-2)  # B, n_cross_prod + n_vectors ,3\n",
    "            \n",
    "            else:\n",
    "                # if no cross products, then the out is simply the vectors ... is there any reason for this function in that case\n",
    "                cat_mat = features[\"vectors\"]  # B, n_vectors_out, 3\n",
    "            \n",
    "            \n",
    "            # matrix multiply the result of output of scalar net and concatenation of input vectors and cross products of vector weights\n",
    "            new_features[\"vectors\"] = torch.bmm(vector_weights, cat_mat) / self.normalize_term\n",
    "\n",
    "        if self.out_dim_s > 0:\n",
    "            new_features[\"scalars\"] = scalar_weights.unsqueeze(-1)    # B, n_scalars_out, 1\n",
    "\n",
    "        return new_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73dc271-e563-4f12-89a5-8259fe9596c9",
   "metadata": {},
   "source": [
    " # Dummy test (equivariant vectors start as zero, so this is meaningless testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "213259eb-f9a8-4460-b67d-42f9b7cfd50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors': tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]], grad_fn=<DivBackward0>),\n",
       " 'scalars': tensor([[[ 0.5766],\n",
       "          [ 0.4713],\n",
       "          [-0.8448],\n",
       "          ...,\n",
       "          [ 0.1207],\n",
       "          [ 0.3421],\n",
       "          [ 0.0924]],\n",
       " \n",
       "         [[ 0.7881],\n",
       "          [-0.0945],\n",
       "          [-1.0130],\n",
       "          ...,\n",
       "          [ 0.0524],\n",
       "          [ 0.2260],\n",
       "          [ 0.1108]],\n",
       " \n",
       "         [[ 1.0134],\n",
       "          [-0.2358],\n",
       "          [-0.6643],\n",
       "          ...,\n",
       "          [-0.6532],\n",
       "          [-0.3257],\n",
       "          [-0.6139]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.5215],\n",
       "          [ 0.1158],\n",
       "          [-0.6790],\n",
       "          ...,\n",
       "          [ 0.1423],\n",
       "          [ 0.0575],\n",
       "          [-0.8263]],\n",
       " \n",
       "         [[ 1.0134],\n",
       "          [-0.2358],\n",
       "          [-0.6643],\n",
       "          ...,\n",
       "          [-0.6532],\n",
       "          [-0.3257],\n",
       "          [-0.6139]],\n",
       " \n",
       "         [[ 0.9722],\n",
       "          [-0.2953],\n",
       "          [-0.8502],\n",
       "          ...,\n",
       "          [-0.6278],\n",
       "          [ 0.2708],\n",
       "          [-0.1491]]], grad_fn=<UnsqueezeBackward0>)}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SO3EquivariantVector(n_features, n_features, n_features, n_features, True, True)(attention_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376358ee-f897-4d37-8997-8dba5b09f4c3",
   "metadata": {},
   "source": [
    " # Used RBF Embedded Writhe as the edge feature in the equivariant attention scheme\n",
    "\n",
    " # Write layer for integration into existing cpaiNN work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "28b0982a-81d9-4be0-816c-f22f2a961691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseSO3Conv(nn.Module):\n",
    "    \n",
    "    \"\"\" Generate pairwise features.\n",
    "    f_ji = h('f_j', 's_j', x_i - x_j, edge_ji)   -> {'vec' , 'scalar'}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_vectors: int,\n",
    "                 n_vectors_out: int,\n",
    "                 n_scalars: int,\n",
    "                 n_scalars_out: int,\n",
    "                 edge_dim: int = int,\n",
    "                 cross_product: bool = True,\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param n_vectors: channels of input vector features\n",
    "        :param n_vectors_out: channels of output vector features\n",
    "        :param n_scalars: channels of input scalar features\n",
    "        :param n_scalars_out: channels of output scalar features\n",
    "        :param edge_dim: dimensions of edge features.\n",
    "        \"\"\"\n",
    "        super(PairwiseSO3Conv, self).__init__()\n",
    "        self.n_vectors = n_vectors\n",
    "        self.n_vectors_out = n_vectors_out\n",
    "        self.n_scalars = n_scalars\n",
    "        self.n_scalars_out = n_scalars_out\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        net_in_vec = n_vectors + 1  # cat(f_j, x_i - x_j)\n",
    "        net_in_s = n_scalars + edge_dim  # cat(s_j, edge_ji)\n",
    "        \n",
    "        self.net = SO3EquivariantVector(net_in_vec,\n",
    "                                        n_vectors_out,\n",
    "                                        net_in_s,\n",
    "                                        n_scalars_out,\n",
    "                                        cross_product)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        \n",
    "        # instantiate total feat dict\n",
    "        input_feat_dict = {}\n",
    "\n",
    "        src_node = batch.edge_index[0]\n",
    "        dst_node = batch.edge_index[1]\n",
    "        \n",
    "        # combine equivariant vectors with displacement vectors, add to total\n",
    "        rel = batch.edge_dir\n",
    "        vec_feats = []\n",
    "        vec_feats.append(batch.equivariant_node_features[src_node])\n",
    "        vec_feats.append(rel[:, None, :])\n",
    "        input_feat_dict['vectors'] = torch.cat(vec_feats, dim=1)  # num_edges, m_in + 1, 3\n",
    "\n",
    "        # combine equivariant scalars with edge features, add to total\n",
    "        add_feat = []\n",
    "        add_feat.append(batch.invariant_node_features[src_node].unsqueeze(-1))\n",
    "        add_feat.append(batch.writhe[src_node].unsqueeze(-1)) #assume we've added writhe\n",
    "        input_feat_dict['scalars'] = torch.cat(add_feat, dim=-2)  # num_edges, n_scalars + edge_dim, 1\n",
    "\n",
    "        # use as input to SE3EquivariantVector network\n",
    "        new_features = self.net(input_feat_dict) # num_edges, m_out, 3\n",
    "\n",
    "        return new_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c4dbc-7520-4485-bec3-8b7a0b085e58",
   "metadata": {},
   "source": [
    "# Make our attention inputs ... these can be simplified / joined in a final function such that the input in simply a batch of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "b07d6588-c51b-460a-a86b-356ea9b33425",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PairwiseSO3Conv(n_features, n_features,n_features,n_features,n_features)(batch)\n",
    "query = SO3EquivariantVector(*(4*[n_features]))(attention_features)\n",
    "value = PairwiseSO3Conv(n_features, n_features,n_features,n_features,n_features)(batch)\n",
    "#attention_module = AttentionModule(1)\n",
    "#key, query = map(attention_module.vectorize_dict, (key, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85853ff-0269-48d5-8795-2852b034858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"An SO(3)-equivariant self-attention module.\"\"\"\n",
    "\n",
    "    def __init__(self, heads: int):\n",
    "        \"\"\"\n",
    "        :param heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.attn_dropout = nn.Dropout(p=0.0)\n",
    "\n",
    "\n",
    "    def vectorize_dict(self, data_dict):\n",
    "        \"\"\"\n",
    "        Vectorize data in the data_dict and concatenate them together.\n",
    "        :param data_dict:\n",
    "            'vec': B, m_vec, 3\n",
    "            'scalar': B, m_s, 1\n",
    "        :return:\n",
    "            B, heads, m_vec // heads * 3 + m_s // heads * 1\n",
    "        \"\"\"\n",
    "        container = []\n",
    "        for key, value in data_dict.items():\n",
    "            B, m_in, dim = value.shape\n",
    "            assert m_in % self.heads == 0, 'm_in is not divisible by heads.'\n",
    "            container.append(value.reshape(B, self.heads, -1))\n",
    "        return torch.cat(container, dim=-1)\n",
    "    \n",
    "    \n",
    "    def forward(self, q: dict, k: dict, v: dict):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param q: dict, query\n",
    "            'vec': B, m_qk_vec, 3\n",
    "            'scalar': B, m_qk_s, 1\n",
    "        :param k: dict, key\n",
    "            'vec': B, m_qk_vec, 3\n",
    "            'scalar': B, m_qk_s, 1\n",
    "        :param v: dict, value\n",
    "            'vec': B, m_v_vec, 3\n",
    "            'scalar': B, m_V_s, 1\n",
    "        :param G:\n",
    "            A DGL graph\n",
    "        :return: dict\n",
    "            {'vec': B, m_v_vec, 3,\n",
    "            'scalar': B, m_v_s, 1}\n",
    "        \"\"\"\n",
    "\n",
    "        src_node, dst_node = batch.edge_index\n",
    "        \n",
    "        query, key = (self.vectorize_dict(i) for i in (q, k))\n",
    "        div_term = math.sqrt(query.shape[-1])\n",
    "        attn = torch.exp((query[dst_node] * key).sum(-1) / div_term).squeeze()\n",
    "        attn = (attn / scatter(attn, dst_node)[dst_node]).reshape(-1, self.heads, 1)\n",
    "\n",
    "        # Apply attention weights to value embeddings\n",
    "        output_dict = {}\n",
    "        for data_type, data_item in v.items():\n",
    "            num_edges, m_in, dim = data_item.shape\n",
    "            assert m_in % self.heads == 0, 'm_in is not divisible by heads in the value embedding.'\n",
    "            weight = data_item.reshape(num_edges, self.heads, -1, dim) * attn.unsqueeze(-1)\n",
    "            new_feature = scatter(weight, dst_node, dim=0)\n",
    "            output_dict[data_type] = new_feature.reshape(-1, m_in, dim)\n",
    "            \n",
    "            return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "02f6681d-884c-41d4-9302-8dc52acf2888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors': tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.3809e-03, -2.7748e-04,  2.0714e-03],\n",
       "          [-1.4380e-03,  2.8893e-04, -2.1569e-03],\n",
       "          [-3.5203e-04,  7.0735e-05, -5.2805e-04],\n",
       "          ...,\n",
       "          [ 6.3171e-04, -1.2693e-04,  9.4756e-04],\n",
       "          [-9.6343e-04,  1.9359e-04, -1.4451e-03],\n",
       "          [ 1.1546e-04, -2.3200e-05,  1.7319e-04]],\n",
       " \n",
       "         [[ 1.6486e-03, -1.1649e-03,  3.9034e-04],\n",
       "          [-1.8759e-03,  1.3749e-03, -3.2096e-04],\n",
       "          [-3.1356e-04,  1.8921e-04, -1.5508e-04],\n",
       "          ...,\n",
       "          [ 1.4566e-03, -1.2362e-03, -1.7207e-04],\n",
       "          [-2.1720e-03,  1.8380e-03,  2.4336e-04],\n",
       "          [-1.6592e-04,  2.0148e-04,  1.7114e-04]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.4192e-04, -2.4147e-04, -6.6787e-04],\n",
       "          [-1.9499e-03,  1.1480e-03,  3.5846e-03],\n",
       "          [ 4.9770e-04, -4.8380e-04, -1.5048e-03],\n",
       "          ...,\n",
       "          [-4.8798e-04,  3.5276e-04,  1.3817e-03],\n",
       "          [-1.7391e-03,  9.3005e-04,  3.4713e-03],\n",
       "          [-7.1759e-04,  5.3063e-04,  1.5371e-03]],\n",
       " \n",
       "         [[ 5.8652e-04, -1.2432e-04, -4.7077e-04],\n",
       "          [-2.8473e-03,  8.3060e-04,  3.2828e-03],\n",
       "          [ 1.0485e-03, -3.9277e-04, -1.4738e-03],\n",
       "          ...,\n",
       "          [-9.1731e-04,  2.8431e-04,  1.3576e-03],\n",
       "          [-2.5892e-03,  6.2397e-04,  3.1596e-03],\n",
       "          [-1.3075e-03,  4.0502e-04,  1.5407e-03]],\n",
       " \n",
       "         [[ 6.1232e-04, -1.3449e-04, -5.2547e-04],\n",
       "          [-3.1310e-03,  8.8423e-04,  3.5322e-03],\n",
       "          [ 1.1930e-03, -4.0773e-04, -1.5652e-03],\n",
       "          ...,\n",
       "          [-1.0928e-03,  3.2741e-04,  1.4749e-03],\n",
       "          [-2.8591e-03,  6.9139e-04,  3.3569e-03],\n",
       "          [-1.4546e-03,  4.2618e-04,  1.6689e-03]]], grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionModule(1)(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440002a4-9d0e-48e4-8e53-9a78b9402b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374879d-b4ce-487d-8c23-cb9a9de3be2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9356f007-25fa-4c8e-8b35-cd8b5f92095b",
   "metadata": {},
   "source": [
    "from dgl import function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "G = to_dgl(batch)\n",
    "\n",
    "with G.local_scope():\n",
    "    G.ndata['query'] = query    # num_nodes, heads, dim\n",
    "    G.edata['key'] = key     # num_edges, heads, dim\n",
    "    div_term = math.sqrt(G.ndata['query'].shape[-1])\n",
    "\n",
    "    # Compute the attention weights\n",
    "    G.apply_edges(fn.e_dot_v('key', 'query', 'attn'))\n",
    "    attn = G.edata.pop('attn') / div_term\n",
    "    attn = edge_softmax(G, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c517cb-ecfe-437f-b6ac-cb319f23e5c0",
   "metadata": {},
   "source": [
    "class LastDimNorm(nn.Module):\n",
    "    def __init__(self, dim: int=3):\n",
    "        super.__init__()\n",
    "        self.norm = nn.InstanceNorm1d(dim)\n",
    "    def forward(self, x):\n",
    "        return norm(vectors.transpose(2, 1)).transpose(2, 1)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
