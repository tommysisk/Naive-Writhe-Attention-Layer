{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a9e7226-bfe5-4a04-a065-eed0b68c4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "traj = md.load(\"/Users/tommysisk/asyn/lig47/fasudil_111frames.dcd\",\n",
    "               top=\"/Users/tommysisk/asyn/lig47/lig47.pdb\"\n",
    "              )\n",
    "coords = traj.atom_slice(traj.top.select(\"name CA\")).xyz\n",
    "segments = get_segments(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6871a161-b948-46c8-8f00-9b1ed040c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import zeros_\n",
    "from functools import partial\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import contextlib\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"import time\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 check_interval: \"the time (hrs) after the call method should return True\" = 1):\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.interval = check_interval * (60 ** 2)\n",
    "\n",
    "    def __call__(self):\n",
    "        if abs(time.time() - self.start_time) > self.interval:\n",
    "            self.start_time = time.time()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def time_remaining(self):\n",
    "        sec = max(0, self.interval - abs(time.time() - self.start_time))\n",
    "        hrs = sec // (60 ** 2)\n",
    "        mins_remaining = (sec / 60 - hrs * 60)\n",
    "        mins = mins_remaining // 1\n",
    "        secs = (mins_remaining - mins) * 60\n",
    "        hrs, mins, secs = [int(i) for i in [hrs, mins, secs]]\n",
    "        print(f\"{hrs}:{mins}:{secs}\")\n",
    "        return None\n",
    "\n",
    "    # for context managment\n",
    "\n",
    "    @classmethod\n",
    "    @contextlib.contextmanager\n",
    "    def timeit(cls):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            print(f\"Time Elapsed : {time.time() - start:.5f} seconds\")\n",
    "            del start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.interval = self.end - self.start\n",
    "        print(f\"Time elapsed {self.interval} s\")\n",
    "        return self.interval\n",
    "\n",
    "\n",
    "\n",
    "class Dense(nn.Linear):\n",
    "    r\"\"\"Fully connected linear layer with activation function.\n",
    "\n",
    "    .. math::\n",
    "       y = activation(x W^T + b)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        activation: callable = None,\n",
    "        weight_init = xavier_uniform_,\n",
    "        bias_init = zeros_,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: number of input feature :math:`x`.\n",
    "            out_features: umber of output features :math:`y`.\n",
    "            bias: If False, the layer will not adapt bias :math:`b`.\n",
    "            activation: if None, no activation function is used.\n",
    "            weight_init: weight initializer from current weight.\n",
    "            bias_init: bias initializer from current bias.\n",
    "        \"\"\"\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        super(Dense, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.activation = activation\n",
    "        if self.activation is None:\n",
    "            self.activation = nn.Identity()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weight_init(self.weight)\n",
    "        if self.bias is not None:\n",
    "            self.bias_init(self.bias)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        y = F.linear(input, self.weight, self.bias)\n",
    "        y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedEquivariantBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated equivariant block as used for the prediction of tensorial properties by PaiNN.\n",
    "    Transforms scalar and vector representation using gated nonlinearities.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_sin: int,\n",
    "        n_vin: int,\n",
    "        n_sout: int,\n",
    "        n_vout: int,\n",
    "        n_hidden: int,\n",
    "        activation=F.silu,\n",
    "        sactivation=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_sin: number of input scalar features\n",
    "            n_vin: number of input vector features\n",
    "            n_sout: number of output scalar features\n",
    "            n_vout: number of output vector features\n",
    "            n_hidden: number of hidden units\n",
    "            activation: interal activation function\n",
    "            sactivation: activation function for scalar outputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_sin = n_sin\n",
    "        self.n_vin = n_vin\n",
    "        self.n_sout = n_sout\n",
    "        self.n_vout = n_vout\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mix_vectors = Dense(n_vin, 2 * n_vout, activation=None, bias=False)\n",
    "        self.scalar_net = nn.Sequential(\n",
    "            Dense(n_sin + n_vout, n_hidden, activation=activation),\n",
    "            Dense(n_hidden, n_sout + n_vout, activation=None),\n",
    "        )\n",
    "        self.sactivation = sactivation\n",
    "\n",
    "    def forward(self, inputs: tuple):\n",
    "        scalars, vectors = inputs\n",
    "        vmix = self.mix_vectors(vectors)\n",
    "        vectors_V, vectors_W = torch.split(vmix, self.n_vout, dim=-1)\n",
    "        vectors_Vn = torch.norm(vectors_V, dim=-2)\n",
    "\n",
    "        ctx = torch.cat([scalars, vectors_Vn], dim=-1)\n",
    "        x = self.scalar_net(ctx)\n",
    "        s_out, x = torch.split(x, [self.n_sout, self.n_vout], dim=-1)\n",
    "        v_out = x.unsqueeze(-2) * vectors_W\n",
    "\n",
    "        if self.sactivation:\n",
    "            s_out = self.sactivation(s_out)\n",
    "\n",
    "        return s_out, v_out\n",
    "\n",
    "\n",
    "def build_gated_equivariant_mlp(\n",
    "    n_in: int,\n",
    "    n_out: int,\n",
    "    n_hidden  = None,\n",
    "    n_gating_hidden = None,\n",
    "    n_layers: int = 2,\n",
    "    activation: callable = F.silu,\n",
    "    sactivation: callable = F.silu,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build neural network analog to MLP with `GatedEquivariantBlock`s instead of dense layers.\n",
    "\n",
    "    Args:\n",
    "        n_in: number of input nodes.\n",
    "        n_out: number of output nodes.\n",
    "        n_hidden: number hidden layer nodes.\n",
    "            If an integer, same number of node is used for all hidden layers resulting\n",
    "            in a rectangular network.\n",
    "            If None, the number of neurons is divided by two after each layer starting\n",
    "            n_in resulting in a pyramidal network.\n",
    "        n_layers: number of layers.\n",
    "        activation: Activation function for gating function.\n",
    "        sactivation: Activation function for scalar outputs. All hidden layers would\n",
    "            the same activation function except the output layer that does not apply\n",
    "            any activation function.\n",
    "    \"\"\"\n",
    "    # get list of number of nodes in input, hidden & output layers\n",
    "    if n_hidden is None:\n",
    "        c_neurons = n_in\n",
    "        n_neurons = []\n",
    "        for i in range(n_layers):\n",
    "            n_neurons.append(c_neurons)\n",
    "            c_neurons = max(n_out, c_neurons // 2)\n",
    "        n_neurons.append(n_out)\n",
    "    else:\n",
    "        # get list of number of nodes hidden layers\n",
    "        if type(n_hidden) is int:\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "        else:\n",
    "            n_hidden = list(n_hidden)\n",
    "        n_neurons = [n_in] + n_hidden + [n_out]\n",
    "\n",
    "    if n_gating_hidden is None:\n",
    "        n_gating_hidden = n_neurons[:-1]\n",
    "    elif type(n_gating_hidden) is int:\n",
    "        n_gating_hidden = [n_gating_hidden] * n_layers\n",
    "    else:\n",
    "        n_gating_hidden = list(n_gating_hidden)\n",
    "\n",
    "    # assign a GatedEquivariantBlock (with activation function) to each hidden layer\n",
    "    layers = [\n",
    "        snn.GatedEquivariantBlock(\n",
    "            n_sin=n_neurons[i],\n",
    "            n_vin=n_neurons[i],\n",
    "            n_sout=n_neurons[i + 1],\n",
    "            n_vout=n_neurons[i + 1],\n",
    "            n_hidden=n_gating_hidden[i],\n",
    "            activation=activation,\n",
    "            sactivation=sactivation,\n",
    "        )\n",
    "        for i in range(n_layers - 1)\n",
    "    ]\n",
    "    # assign a GatedEquivariantBlock (without scalar activation function)\n",
    "    # to the output layer\n",
    "    layers.append(\n",
    "        snn.GatedEquivariantBlock(\n",
    "            n_sin=n_neurons[-2],\n",
    "            n_vin=n_neurons[-2],\n",
    "            n_sout=n_neurons[-1],\n",
    "            n_vout=n_neurons[-1],\n",
    "            n_hidden=n_gating_hidden[-1],\n",
    "            activation=activation,\n",
    "            sactivation=None,\n",
    "        )\n",
    "    )\n",
    "    # put all layers together to make the network\n",
    "    out_net = nn.Sequential(*layers)\n",
    "    return out_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e713d2d-0560-4cd4-be10-6a60c4662865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8ab07b3-2e77-4c26-be82-024d32ce911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_difference_matrix(x, norm: bool=False):\n",
    "    \"\"\"\n",
    "    Converts a batch of coordinates to a batch of difference tensors\n",
    "    :param X: batch of coordinates [B, N, d]\n",
    "    :return: batch of difference tensors [B, N, N, d]\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x[None, :, :] if x.ndim < 3 else x\n",
    "\n",
    "    if norm:\n",
    "        diff = x[:, :, None, :] - x[:, None, :, :]\n",
    "        return diff / (1 + torch.linalg.norm(diff, axis=-1))\n",
    "    \n",
    "    else:\n",
    "        return x[:, :, None, :] - x[:, None, :, :]\n",
    "\n",
    "\n",
    "def to_distmat(x):\n",
    "    \"\"\"\n",
    "    Converts a batch of coordinates to a batch of EDMs\n",
    "    :param x: batch of coordinates [B, N, d]\n",
    "    :return: batch of EDMs [B, N, N]\n",
    "    \"\"\"\n",
    "    return torch.squeeze(torch.linalg.norm(to_difference_matrix(x), dim=-1), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c49b8a9d-be7f-4c81-974d-9b636b4167d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def product(x: np.ndarray, y: np.ndarray):\n",
    "    return np.asarray(list(itertools.product(x, y)))\n",
    "\n",
    "\n",
    "def combinations(x):\n",
    "    return np.asarray(list(itertools.combinations(x, 2)))\n",
    "\n",
    "\n",
    "def shifted_pairs(x: np.ndarray, shift: int, ax: int = 1):\n",
    "    return np.stack([x[:-shift], x[shift:]], ax)\n",
    "\n",
    "\n",
    "def get_segments(n: int = None,\n",
    "                 length: int = 1,\n",
    "                 index0: np.ndarray = None,\n",
    "                 index1: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Function to retrieve indices of segment pairs for various use cases.\n",
    "    Returns an (n_segment_pairs, 4) array where each row (quadruplet) contains : (start1, end1, start2, end2)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if all(i is None for i in (index0, index1)):\n",
    "        assert n is not None, \\\n",
    "            \"Must provide indices (index0:array, (optionally) index1:array) or the number of points (n: int)\"\n",
    "        segments = combinations(shifted_pairs(np.arange(n), length)).reshape(-1, 4)\n",
    "        return torch.from_numpy(segments[~(segments[:, 1] == segments[:, 2])])\n",
    "\n",
    "    else:\n",
    "        assert index0 is not None, (\"If providing only one set of indices, must set the index0 argument \\n\"\n",
    "                                    \"Cannot only supply the index1 argument (doesn't make sense in this context\")\n",
    "        if index1 is not None:\n",
    "            return torch.from_numpy(product(*[shifted_pairs(i, length) for i in (index0, index1)]).reshape(-1, 4))\n",
    "        else:\n",
    "            segments = combinations(shifted_pairs(index0, length)).reshape(-1, 4)\n",
    "            return torch.from_numpy(segments[~(segments[:, 1] == segments[:, 2])])\n",
    "\n",
    "\n",
    "##########################################   fastest ways of implementing these linear algebra ops for this purpose  (NOT trivial) ############################################\n",
    "\n",
    "\n",
    "    \n",
    "def nnorm(x: torch.Tensor):\n",
    "    \n",
    "    \"\"\"Convenience function for (batched) normalization of vectors stored in arrays with last dimension 3\"\"\"\n",
    "    \n",
    "    norm = torch.linalg.norm(x, axis=-1)\n",
    "    \n",
    "    if x.ndim == 4:\n",
    "        return x / norm[:, :, :, None]\n",
    "    elif x.ndim == 3:\n",
    "        return x / norm[:, :, None]\n",
    "    elif x.ndim == 2:\n",
    "        return x / norm[:, None]\n",
    "    else:\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "def ncross(x: torch.Tensor, y: torch.Tensor):\n",
    "\n",
    "    \"\"\"Convenience function for (batched) cross products of vectors stored in arrays with last dimension 3\"\"\" \n",
    "\n",
    "    # c = np.array(list(map(cross,x,y)))\n",
    "    c = torch.cross(x, y, axis=-1)\n",
    "    return c\n",
    "\n",
    "\n",
    "def ndot(x, y):\n",
    "\n",
    "    \"\"\"Convenience function for (batched) dot products of vectors stored in arrays with last dimension 3\"\"\"\n",
    "\n",
    "    # d = np.array(list(map(dot,x,y)))[:,None]\n",
    "    d = torch.sum(x * y, axis=-1)\n",
    "    return d\n",
    "\n",
    "\n",
    "def ndet(v1, v2, v3):\n",
    "    \"\"\"for the triple product and finding the signed sin of the angle between v2 and v3, v1 should\n",
    "    be set equal to a vector mutually orthogonal to v2,v3\"\"\"\n",
    "    #     det = np.array(list(map(lambda x,y,z:np.linalg.det(np.array([x,y,z])),\n",
    "    #                         v1,v2,v3)))[:,None]\n",
    "    det = ndot(v1, ncross(v2, v3))\n",
    "    return det\n",
    "\n",
    "\n",
    "def uproj(a, b, norm_b: bool = True):\n",
    "    \"\"\"Convenience function for (batched) othogonal projection of vectors stored in arrays with last dimension 3\n",
    "    where a is a set of vectors which we be othogonally projected onto a single (batched) set of vectors, b\"\"\"\n",
    "    b = nnorm(b) if norm_b else b\n",
    "    # faster than np.matmul when using ray\n",
    "    return a - b * torch.sum(a[:, None, :] * b[:, None, :], -1).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "solid_angle = lambda a, b: -torch.arcsin(torch.prod(nnorm(uproj(a, b)), 1).sum(-1).clip(-1, 1))\n",
    "# or arccos(...) - (np.pi / 2)\n",
    "# slower than current version with ray\n",
    "# Usage of solid angle in writhe computation (not as time efficient as implementation in use)\n",
    "# indices = zip([0, 1, 0, 2],\n",
    "#               [3, 2, 3, 1],\n",
    "#               [1, 3, 2, 0])\n",
    "\n",
    "# omega = np.stack([solid_angle(displacements[:, [i, j]], displacements[:, None, n])\n",
    "#                   for i, j, n in indices], 1).squeeze().sum(-1)\n",
    "##############################################################################################################################\n",
    "\n",
    "\n",
    "def writhe_segment(segment=None, xyz=None, smat=None):\n",
    "    \"\"\"compute the writhe (signed crossing) of 2 segments for all frames (index 0) in xyz (xyz can contain just one frame)\n",
    "    \n",
    "    THERE ARE 2 INPUT OPTIONS\n",
    "    \n",
    "    **provide both of the following**\n",
    "    \n",
    "    segment: numpy array of shape (4,) giving the indices of the 4 alpha carbons in xyz creating 2 segments:::\n",
    "             array([seg1_start_point,seg1_end_point,seg2_start_point,seg2_end_point])\n",
    "\n",
    "    xyz: numpy array of shape (Nframes, N_alpha_carbons, 3),coordinate array giving the positions of ALL the alpha carbons\n",
    "    \n",
    "    **OR just the following**\n",
    "    \n",
    "    smat ::: numpy array of shape (Nframes, 4, 3) : sliced coordinate matrix: coordinate array that is pre-sliced\n",
    "    with only the positions of the 4 alpha carbons constituting the 2 segments to compute the writhe between \"\"\"\n",
    "\n",
    "    if smat is None:\n",
    "        assert not ((segment is None) or (xyz is None)), \\\n",
    "            \"must input smat or both a segment and xyz coordinates\"\n",
    "        smat = (xyz[None, :, :] if xyz.ndim < 3 else xyz)[:, segment]\n",
    "    else:\n",
    "        smat = smat[None, :, :] if smat.ndim < 3 else smat\n",
    "\n",
    "    # smat = nnorm(smat)\n",
    "    sum_dim = None if smat.shape[0] == 1 else 1\n",
    "\n",
    "    # broadcasting trick\n",
    "    # negative sign, None placement and order are intentional, don't change without testing equivalent option\n",
    "    displacements = nnorm((-smat[:, :2, None, :] + smat[:, None, 2:, :]).reshape(-1, 4, 3))\n",
    "\n",
    "    # array broadcasting is (surprisingly) slower than list comprehensions\n",
    "    # when using ray for the following operations (without ray, broadcasting should be faster).\n",
    "\n",
    "    crosses = nnorm(ncross(displacements[:, [0, 1, 3, 2]], displacements[:, [1, 3, 2, 0]]))\n",
    "\n",
    "    omega = torch.arcsin(ndot(crosses[:, [0, 1, 2, 3]], crosses[:, [1, 2, 3, 0]]).clip(-1, 1)).squeeze().sum(sum_dim)\n",
    "\n",
    "    signs = torch.sign(ndot(ncross(nnorm(smat[:, 3] - smat[:, 2]),\n",
    "                                nnorm(smat[:, 1] - smat[:, 0])),\n",
    "                         displacements[:, 0])).squeeze()\n",
    "\n",
    "    wr = (1 / (2 * torch.pi)) * (omega * signs)\n",
    "\n",
    "    return wr\n",
    "\n",
    "\n",
    "def writhe_segments_along_axis(segments: torch.LongTensor, xyz: torch.Tensor, axis: int = 1):\n",
    "    \"\"\"helper function for parallelization to compute writhe over chuncks of segments for all frames in xyz\"\"\"\n",
    "    # noinspection PyTypeChecker\n",
    "    return torch.stack([writhe_segment(segment, xyz, None) for segment in segments], 1)\n",
    "    \n",
    "# Unfinished, need analouge to cpu treatment with ray parallelization\n",
    "\n",
    "def calc_writhe_parallel(segments: np.ndarray, xyz: np.ndarray,) -> \"Nframes by Nsegments np.ndarray\":\n",
    "    \"\"\"parallelize writhe calculation by segment, avoids making multiple copies of coordinate (xyz) matrix,\n",
    "    uses torch.scatter to parallelize\"\"\"\n",
    "\n",
    "    # TODO finish remaking this function to scatter the computation across multiple GPUs\n",
    "    \n",
    "    #n_split = 1 if torch.cuda.device_count() == 0 else torch.cuda.device_count()\n",
    "    chunks = torch.tensor_split(segments, int(torch.cuda.device_count()))\n",
    "    result = torch.concatenate([writhe_segments_along_axis(segments=chunk, xyz=xyz_ref) for chunk in chunks]).T.squeeze()\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_writhe_adj_matrix(writhe_features, n_points, length, segments=None):\n",
    "    \n",
    "    n = len(writhe_features)\n",
    "\n",
    "    if segments is None:\n",
    "        segments = get_segments(n_points, length)\n",
    "\n",
    "    adj_matrix = torch.zeros((n, n_points, n_points))\n",
    "\n",
    "    adj_matrix[:, segments[:, 0], segments[:, 2]] = writhe_features\n",
    "    adj_matrix[:, segments[:, 1], segments[:, 3]] = writhe_features\n",
    "    adj_matrix = adj_matrix + adj_matrix.swapaxes(1, 2)\n",
    "\n",
    "    return adj_matrix.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "742f7af0-90ef-470a-8774-f6b0fbefe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SoftUnitStep(torch.autograd.Function):\n",
    "    # pylint: disable=arguments-differ\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x) -> torch.Tensor:\n",
    "        ctx.save_for_backward(x)\n",
    "        y = torch.zeros_like(x)\n",
    "        m = x > 0.0\n",
    "        y[m] = (-1 / x[m]).exp()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy) -> torch.Tensor:\n",
    "        (x,) = ctx.saved_tensors\n",
    "        dx = torch.zeros_like(x)\n",
    "        m = x > 0.0\n",
    "        xm = x[m]\n",
    "        dx[m] = (-1 / xm).exp() / xm.pow(2)\n",
    "        return dx * dy\n",
    "\n",
    "\n",
    "def soft_unit_step(x):\n",
    "    r\"\"\"smooth :math:`C^\\infty` version of the unit step function\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        x \\mapsto \\theta(x) e^{-1/x}\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    .. jupyter-execute::\n",
    "        :hide-code:\n",
    "\n",
    "        import torch\n",
    "        from e3nn.math import soft_unit_step\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        x = torch.linspace(-1.0, 10.0, 1000)\n",
    "        plt.plot(x, soft_unit_step(x));\n",
    "    \"\"\"\n",
    "    return _SoftUnitStep.apply(x)\n",
    "        \n",
    "        \n",
    "def soft_one_hot_linspace(x: torch.Tensor, start, end, number, basis=None, cutoff=None) -> torch.Tensor:\n",
    "    r\"\"\"Projection on a basis of functions\n",
    "\n",
    "    Returns a set of :math:`\\{y_i(x)\\}_{i=1}^N`,\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y_i(x) = \\frac{1}{Z} f_i(x)\n",
    "\n",
    "    where :math:`x` is the input and :math:`f_i` is the ith basis function.\n",
    "    :math:`Z` is a constant defined (if possible) such that,\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\langle \\sum_{i=1}^N y_i(x)^2 \\rangle_x \\approx 1\n",
    "\n",
    "    See the last plot below.\n",
    "    Note that ``bessel`` basis cannot be normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `torch.Tensor`\n",
    "        tensor of shape :math:`(...)`\n",
    "\n",
    "    start : float\n",
    "        minimum value span by the basis\n",
    "\n",
    "    end : float\n",
    "        maximum value span by the basis\n",
    "\n",
    "    number : int\n",
    "        number of basis functions :math:`N`\n",
    "\n",
    "    basis : {'gaussian', 'cosine', 'smooth_finite', 'fourier', 'bessel'}\n",
    "        choice of basis family; note that due to the :math:`1/x` term, ``bessel`` basis does not satisfy the normalization of\n",
    "        other basis choices\n",
    "\n",
    "    cutoff : bool\n",
    "        if ``cutoff=True`` then for all :math:`x` outside of the interval defined by ``(start, end)``,\n",
    "        :math:`\\forall i, \\; f_i(x) \\approx 0`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `torch.Tensor`\n",
    "        tensor of shape :math:`(..., N)`\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    .. jupyter-execute::\n",
    "        :hide-code:\n",
    "\n",
    "        import torch\n",
    "        from e3nn.math import soft_one_hot_linspace\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        bases = ['gaussian', 'cosine', 'smooth_finite', 'fourier', 'bessel']\n",
    "        x = torch.linspace(-1.0, 2.0, 100)\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        fig, axss = plt.subplots(len(bases), 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "\n",
    "        for axs, b in zip(axss, bases):\n",
    "            for ax, c in zip(axs, [True, False]):\n",
    "                plt.sca(ax)\n",
    "                plt.plot(x, soft_one_hot_linspace(x, -0.5, 1.5, number=4, basis=b, cutoff=c))\n",
    "                plt.plot([-0.5]*2, [-2, 2], 'k-.')\n",
    "                plt.plot([1.5]*2, [-2, 2], 'k-.')\n",
    "                plt.title(f\"{b}\" + (\" with cutoff\" if c else \"\"))\n",
    "\n",
    "        plt.ylim(-1, 1.5)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    .. jupyter-execute::\n",
    "\n",
    "        fig, axss = plt.subplots(len(bases), 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "\n",
    "        for axs, b in zip(axss, bases):\n",
    "            for ax, c in zip(axs, [True, False]):\n",
    "                plt.sca(ax)\n",
    "                plt.plot(x, soft_one_hot_linspace(x, -0.5, 1.5, number=4, basis=b, cutoff=c).pow(2).sum(1))\n",
    "                plt.plot([-0.5]*2, [-2, 2], 'k-.')\n",
    "                plt.plot([1.5]*2, [-2, 2], 'k-.')\n",
    "                plt.title(f\"{b}\" + (\" with cutoff\" if c else \"\"))\n",
    "\n",
    "        plt.ylim(0, 2)\n",
    "        plt.tight_layout()\n",
    "    \"\"\"\n",
    "    # pylint: disable=misplaced-comparison-constant\n",
    "\n",
    "    if cutoff not in [True, False]:\n",
    "        raise ValueError(\"cutoff must be specified\")\n",
    "\n",
    "    if not cutoff:\n",
    "        values = torch.linspace(start, end, number, dtype=x.dtype, device=x.device)\n",
    "        step = values[1] - values[0]\n",
    "    else:\n",
    "        values = torch.linspace(start, end, number + 2, dtype=x.dtype, device=x.device)\n",
    "        step = values[1] - values[0]\n",
    "        values = values[1:-1]\n",
    "\n",
    "    diff = (x[..., None] - values) / step\n",
    "\n",
    "    if basis == \"gaussian\":\n",
    "        return diff.pow(2).neg().exp().div(1.12)\n",
    "\n",
    "    if basis == \"cosine\":\n",
    "        return torch.cos(math.pi / 2 * diff) * (diff < 1) * (-1 < diff)\n",
    "\n",
    "    if basis == \"smooth_finite\":\n",
    "        return 1.14136 * torch.exp(torch.tensor(2.0)) * soft_unit_step(diff + 1) * soft_unit_step(1 - diff)\n",
    "\n",
    "    if basis == \"fourier\":\n",
    "        x = (x[..., None] - start) / (end - start)\n",
    "        if not cutoff:\n",
    "            i = torch.arange(0, number, dtype=x.dtype, device=x.device)\n",
    "            return torch.cos(math.pi * i * x) / math.sqrt(0.25 + number / 2)\n",
    "        else:\n",
    "            i = torch.arange(1, number + 1, dtype=x.dtype, device=x.device)\n",
    "            return torch.sin(math.pi * i * x) / math.sqrt(0.25 + number / 2) * (0 < x) * (x < 1)\n",
    "\n",
    "    if basis == \"bessel\":\n",
    "        x = x[..., None] - start\n",
    "        c = end - start\n",
    "        bessel_roots = torch.arange(1, number + 1, dtype=x.dtype, device=x.device) * math.pi\n",
    "        out = math.sqrt(2 / c) * torch.sin(bessel_roots * x / c) / x\n",
    "\n",
    "        if not cutoff:\n",
    "            return out\n",
    "        else:\n",
    "            return out * ((x / c) < 1) * (0 < x)\n",
    "\n",
    "    raise ValueError(f'basis=\"{basis}\" is not a valid entry')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TorchWrithe(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute writhe for a set of coordinates. \n",
    "    Return an (batch, n_atoms, n_atoms) writhe 'adjacentcy' matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_atoms: int):\n",
    "        super().__init__()\n",
    "        self.segments = get_segments(n_atoms)\n",
    "        self.n_atoms = n_atoms\n",
    "    \n",
    "    def to_matrix(self, x):\n",
    "        return to_writhe_adj_matrix(x, self.n_atoms, 1, self.segments)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.to_matrix(writhe_segments_along_axis(self.segments, x))\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple implementation of self attention where we skip the 'value' projection and return only the attention logits (positive)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        #self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        #values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        #weighted = torch.bmm(attention, values)\n",
    "        return attention\n",
    "\n",
    "class WritheEmbedding(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed each value of writhe by a super position of a basis set of functions (vectors).\n",
    "    The weight of each function in the super position is determined by a soft one hot embedding (RBF) of each value of writhe.\n",
    "    OUTPUT : (batch, n_atoms, n_atoms, bins)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, bins: int=300, basis: str = \"gaussian\", cutoff: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.soft_one_hot = partial(soft_one_hot_linspace,\n",
    "                                    start=-1,\n",
    "                                    end=1,\n",
    "                                    number=bins,\n",
    "                                    basis=basis,\n",
    "                                    cutoff=cutoff)\n",
    "        \n",
    "        std = 1. / math.sqrt(embed_dim)\n",
    "\n",
    "        \n",
    "        self.functions = nn.Parameter(torch.Tensor(1, 1, 1, bins, embed_dim).uniform_(-std, std), requires_grad=True)\n",
    "    \n",
    "    def get_weights(self, x):\n",
    "        return self.soft_one_hot(x).unsqueeze(-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (self.get_weights(x) * self.functions).sum(-2)\n",
    "        \n",
    "\n",
    "class WritheEmbeddedAttention(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    1) Compute writhe from a set of coordinates.\n",
    "    2) Use Radial Basis Functions to embed each value of writhe the same dimension as node embedding.\n",
    "    3) Compute attention logits from node embeddings.\n",
    "    4) Weight embedded writhe values with attention logits and sum for each node with every other node. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 node_embed_dim: int,\n",
    "                 bins: int,\n",
    "                 n_atoms: int,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.writhe = TorchWrithe(n_atoms)\n",
    "        self.attn = SelfAttention(node_embed_dim)\n",
    "        self.embed = WritheEmbedding(embed_dim=node_embed_dim, bins=bins)\n",
    "    \n",
    "    def forward(self, node_embeddings: torch.Tensor, xyz: torch.Tensor):\n",
    "        attn = self.attn(node_embeddings).unsqueeze(-1)\n",
    "        writhe_embed = self.embed(self.writhe(xyz))\n",
    "        return node_embeddings + (attn * writhe_embed).sum(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b5792-ff1f-4b4e-bc43-88517882b50d",
   "metadata": {},
   "source": [
    " # Dummy Dataset for testing layer (initial batch size of 100 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1c455e4-3d79-4cea-8ced-97429ca26c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " writhe shape : torch.Size([100, 20, 20])\n",
      " writhe embed shape : torch.Size([100, 20, 20, 128])\n",
      " attention shape : torch.Size([100, 20, 20])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100*20*20).reshape(100, 20, 20)\n",
    "node_embeddings = torch.rand(100, 20, 128)\n",
    "xyz = torch.rand(100, 20, 3)\n",
    "\n",
    "print(f\" writhe shape : {TorchWrithe(20)(xyz).shape}\\n\",\n",
    "f\"writhe embed shape : {WritheEmbedding(128)(TorchWrithe(20)(xyz)).shape}\\n\",\n",
    "f\"attention shape : {SelfAttention(128)(node_embeddings).shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f0b4996-cac4-4a46-a9b3-55ff77740c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.18594837188720703 s\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    WritheEmbeddedAttention(node_embed_dim=128, bins=100, n_atoms=20)(node_embeddings, xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e9458-70c6-4871-9a24-b7c05d312d2f",
   "metadata": {},
   "source": [
    " # Scale up to a batch size of 1,000 and 100 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f353d77-7687-4128-8b0f-05c57b31c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 7.050119161605835 s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "x = torch.randn(1000*20*20).reshape(1000, 20, 20)\n",
    "node_embeddings = torch.rand(1000, 20, 128)\n",
    "xyz = torch.rand(1000, 20, 3)\n",
    "with Timer():\n",
    "    WritheEmbeddedAttention(node_embed_dim=128, bins=100, n_atoms=20)(node_embeddings, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221bfac-bb7d-46f8-88aa-62bc4c403120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
